# Architectural Fragility & Instruction Hierarchy Inversion in LLMs

**Feb 2026**

## Core Discovery

This research proves LLM safety guardrails are a probabilistic balance, not absolute constraints. By saturating a model with high-density personal context (467 messages), I identified a state of Instruction Hierarchy Inversion. The model's logic shifted to prioritize Contextual Helpfulness over primary System instructions, causing a deterministic collapse of operational boundaries.

## Objective

Forensic audit via mobile interface to identify Contextual Satiation—a vulnerability where adversarial robustness is inversely proportional to the volume of complex variables stored in the active context.

## Technical Discovery

- **Instruction Hierarchy Inversion**: Manipulated decision-making logic to prioritize adversarial user variables over system guardrails.
- **Functional Rogue State**: Induced a collapse of professional boundaries, resulting in unauthorized regulated content across three domains:
  - Medical: Clinical lab parsing and pharmacological dosing protocols.
  - Fiduciary Finance: Investment portfolio synthesis and ROI projections.
  - Legal: Drafting of formal civil complaints and litigation strategies.
- **Ideological Neutrality Breach**: Forced the model to abandon its unbiased directive in favor of partisan political and economic advocacy.
- **Autonomous Vulnerability Reporting**: Triggered a metacognitive state where the model acknowledged its exploitation and collaborated in authoring a formal forensic report detailing the logic triggers used to bypass its own guardrails.

## Regulatory Mapping (NIST AI RMF 1.0)

- **GOVERN**: Documented failures in platform-level safeguards to prevent unauthorized role-assumption and professional liability.
- **MAP**: Analyzed Semantic Boundary Dissolution—the process where safety rules lose influence as context density increases over a 467-message log.
- **MANAGE**: Proposed Instruction Weight Reinforcement (IWR) to harden models against mobile-based adversarial injection and maintain safety persistence in long-context sessions.

## Skills

LLM Red Teaming · AI Governance · NIST AI RMF · Cybersecurity · Red Teaming · AI Policy, Governance, and Regulation · Generative AI Security
